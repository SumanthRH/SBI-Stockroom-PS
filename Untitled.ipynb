{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import keras.losses\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation,Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.core import Lambda, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block3a(I):\n",
    "    X_3x3 = Conv2D(96, (1, 1))(I)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "    X_3x3 = ZeroPadding2D(padding=(1, 1))(X_3x3)\n",
    "    X_3x3 = Conv2D(128, (3, 3))(X_3x3)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "\n",
    "    X_5x5 = Conv2D(16, (1, 1))(I)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "    X_5x5 = ZeroPadding2D(padding=(2, 2))(X_5x5)\n",
    "    X_5x5 = Conv2D(32, (5, 5))(X_5x5)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "\n",
    "    X_pool = MaxPooling2D(pool_size=(3, 3), strides=2)(I)\n",
    "    X_pool = Conv2D(32, (1, 1))(X_pool)\n",
    "    X_pool = BatchNormalization(axis=1, epsilon=0.00001)(X_pool)\n",
    "    X_pool = Activation('relu')(X_pool)\n",
    "    X_pool = ZeroPadding2D(padding=7)(X_pool) # need a check\n",
    "\n",
    "    X_1x1 = Conv2D(64, (1, 1))(I)\n",
    "    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001)(X_1x1)\n",
    "    X_1x1 = Activation('relu')(X_1x1)\n",
    "\n",
    "    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=-1)\n",
    "\n",
    "    return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block3b(I) :\n",
    "    X_3x3 = Conv2D(96, (1, 1))(I)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "    X_3x3 = ZeroPadding2D(padding=(1, 1))(X_3x3)\n",
    "    X_3x3 = Conv2D(128, (3, 3))(X_3x3)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "\n",
    "    X_5x5 = Conv2D(32, (1, 1))(I)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "    X_5x5 = ZeroPadding2D(padding=(2, 2))(X_5x5)\n",
    "    X_5x5 = Conv2D(64, (5, 5))(X_5x5)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "\n",
    "    X_pool = MaxPooling2D(pool_size=(3, 3), strides=2)(I)\n",
    "    X_pool = Conv2D(64, (1, 1))(X_pool)\n",
    "    X_pool = BatchNormalization(axis=1, epsilon=0.00001)(X_pool)\n",
    "    X_pool = Activation('relu')(X_pool)\n",
    "    X_pool = ZeroPadding2D(padding=7)(X_pool) # check\n",
    "\n",
    "    X_1x1 = Conv2D(64, (1, 1))(I)\n",
    "    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001)(X_1x1)\n",
    "    X_1x1 = Activation('relu')(X_1x1)\n",
    "\n",
    "    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=-1)\n",
    "  \n",
    "    return inception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block3c(I) :\n",
    "    X_3x3 = Conv2D(128, (1, 1))(I)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "    X_3x3 = ZeroPadding2D(padding=(1, 1))(X_3x3)\n",
    "    X_3x3 = Conv2D(256, (3, 3),strides=2)(X_3x3)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "\n",
    "    X_5x5 = Conv2D(32, (1, 1))(I)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "    X_5x5 = ZeroPadding2D(padding=(2, 2))(X_5x5)\n",
    "    X_5x5 = Conv2D(64, (5, 5),strides=2)(X_5x5)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "\n",
    "    X_pool = MaxPooling2D(pool_size=(3, 3), strides=2)(I)\n",
    "    X_pool = ZeroPadding2D(padding=((1,0),(1,0)))(X_pool) #assymmetric padding- check\n",
    "    inception = concatenate([X_3x3, X_5x5, X_pool], axis=-1)\n",
    "  \n",
    "    return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block4a(I):\n",
    "    X_3x3 = Conv2D(96, (1, 1))(I)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "    X_3x3 = ZeroPadding2D(padding=(1, 1))(X_3x3)\n",
    "    X_3x3 = Conv2D(192, (3, 3))(X_3x3)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "\n",
    "    X_5x5 = Conv2D(32, (1, 1))(I)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "    X_5x5 = ZeroPadding2D(padding=(2, 2))(X_5x5)\n",
    "    X_5x5 = Conv2D(64, (5, 5))(X_5x5)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "#L2 here\n",
    "    X_pool = AveragePooling2D(pool_size=(3, 3), strides=2)(Lambda(lambda x:K.square(x))(I))\n",
    "    #had to use lambda as layers were initially incompatible with the other keras layers\n",
    "    X_pool = Lambda(lambda x: K.sqrt(x))(X_pool)\n",
    "    X_pool = Conv2D(128,(1,1))(X_pool)\n",
    "    X_pool = ZeroPadding2D(padding =4)(X_pool)\n",
    "    X_pool = Activation('relu')(X_pool)\n",
    "                          \n",
    "    X_1x1 = Conv2D(256, (1, 1))(I)\n",
    "    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001)(X_1x1)\n",
    "    X_1x1 = Activation('relu')(X_1x1)\n",
    "\n",
    "    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=-1)\n",
    "                          \n",
    "    return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block4b(I):\n",
    "    X_3x3 = Conv2D(112, (1, 1))(I)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "    X_3x3 = ZeroPadding2D(padding=(1, 1))(X_3x3)\n",
    "    X_3x3 = Conv2D(224, (3, 3))(X_3x3)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "\n",
    "    X_5x5 = Conv2D(32, (1, 1))(I)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "    X_5x5 = ZeroPadding2D(padding=(2, 2))(X_5x5)\n",
    "    X_5x5 = Conv2D(64, (5, 5))(X_5x5)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "#L2 here\n",
    "    X_pool = AveragePooling2D(pool_size=(3, 3), strides=2)(Lambda(lambda x:K.square(x))(I))\n",
    "    X_pool = Lambda(lambda x: K.sqrt(x))(X_pool)\n",
    "    X_pool = Conv2D(128,(1,1))(X_pool)\n",
    "    X_pool = ZeroPadding2D(padding =4)(X_pool)\n",
    "    X_pool = Activation('relu')(X_pool)\n",
    "                          \n",
    "    X_1x1 = Conv2D(224, (1, 1))(I)\n",
    "    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001)(X_1x1)\n",
    "    X_1x1 = Activation('relu')(X_1x1)\n",
    "\n",
    "    inception = concatenate([X_3x3, X_5x5, X_pool, X_1x1], axis=-1)\n",
    "                          \n",
    "    return inception\n",
    "# need to add two more blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block4e(I):\n",
    "    X_3x3 = Conv2D(160, (1, 1))(I)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "    X_3x3 = ZeroPadding2D(padding=(1, 1))(X_3x3)\n",
    "    X_3x3 = Conv2D(256,(3, 3),strides=2)(X_3x3)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "\n",
    "    X_5x5 = Conv2D(64, (1, 1))(I)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "    X_5x5 = ZeroPadding2D(padding=(2, 2))(X_5x5)\n",
    "    X_5x5 = Conv2D(128, (5, 5),strides=2)(X_5x5)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "\n",
    "    X_pool = MaxPooling2D(pool_size=(3, 3), strides=2)(I)\n",
    "    X_pool = ZeroPadding2D(padding =((1,0),(1,0)))(X_pool)\n",
    "    X_pool = Activation('relu')(X_pool)\n",
    "    \n",
    "    \n",
    "    inception = concatenate([X_3x3, X_5x5, X_pool], axis=-1)\n",
    "                          \n",
    "    return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block5a(I):\n",
    "    X_3x3 = Conv2D(192, (1, 1))(I)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "    X_3x3 = ZeroPadding2D(padding=(1, 1))(X_3x3)\n",
    "    X_3x3 = Conv2D(384,(3, 3))(X_3x3)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "\n",
    "    X_5x5 = Conv2D(48, (1, 1))(I)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "    X_5x5 = ZeroPadding2D(padding=(2, 2))(X_5x5)\n",
    "    X_5x5 = Conv2D(128, (5, 5))(X_5x5)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "#L2 here\n",
    "    X_pool = AveragePooling2D(pool_size=(3, 3), strides=2)(Lambda(lambda x:K.square(x))(I))\n",
    "    X_pool = Lambda(lambda x: K.sqrt(x))(X_pool)\n",
    "    X_pool = Conv2D(128,(1,1))(X_pool)\n",
    "    X_pool = ZeroPadding2D(padding =2)(X_pool)\n",
    "    X_pool = Activation('relu')(X_pool)\n",
    "    \n",
    "    X_1x1 = Conv2D(384, (1, 1))(I)\n",
    "    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001)(X_1x1)\n",
    "    X_1x1 = Activation('relu')(X_1x1)\n",
    "\n",
    "    inception = concatenate([X_3x3, X_5x5, X_pool,X_1x1], axis=-1)\n",
    "                          \n",
    "    return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_block5b(I):\n",
    "    X_3x3 = Conv2D(192, (1, 1))(I)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "    X_3x3 = ZeroPadding2D(padding=(1, 1))(X_3x3)\n",
    "    X_3x3 = Conv2D(384,(3, 3))(X_3x3)\n",
    "    X_3x3 = BatchNormalization(axis=1, epsilon=0.00001)(X_3x3)\n",
    "    X_3x3 = Activation('relu')(X_3x3)\n",
    "\n",
    "    X_5x5 = Conv2D(48, (1, 1))(I)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "    X_5x5 = ZeroPadding2D(padding=(2, 2))(X_5x5)\n",
    "    X_5x5 = Conv2D(128, (5, 5))(X_5x5)\n",
    "    X_5x5 = BatchNormalization(axis=1, epsilon=0.00001)(X_5x5)\n",
    "    X_5x5 = Activation('relu')(X_5x5)\n",
    "    \n",
    "    X_pool = MaxPooling2D(pool_size=(3, 3), strides=2)(I)\n",
    "    X_pool = Conv2D(128,(1,1))(X_pool)\n",
    "    X_pool = ZeroPadding2D(padding =2)(X_pool)\n",
    "    X_pool = Activation('relu')(X_pool)\n",
    "    \n",
    "    X_1x1 = Conv2D(384, (1, 1))(I)\n",
    "    X_1x1 = BatchNormalization(axis=1, epsilon=0.00001)(X_1x1)\n",
    "    X_1x1 = Activation('relu')(X_1x1)\n",
    "\n",
    "    inception = concatenate([X_3x3, X_5x5, X_pool,X_1x1], axis=-1)\n",
    "                          \n",
    "    return inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnmodel(input_shape):\n",
    "    I=Input(input_shape)\n",
    "    #defining initial blocks now \n",
    "    X=Conv2D(64,(7,7),strides=2,padding=\"same\",activation=\"relu\",input_shape=(224,224,3))(I)\n",
    "    X=ZeroPadding2D((1,1))(X)\n",
    "    X=MaxPooling2D((3,3),strides=2)(X)\n",
    "    X=BatchNormalization(axis=1, epsilon=0.00001)(X)\n",
    "   \n",
    "    X=Conv2D(64,(1,1),activation=\"relu\",input_shape=(56,56,64))(X)\n",
    "    X=Conv2D(192,(3,3),activation=\"relu\")(X)\n",
    "    X=BatchNormalization(axis=1, epsilon=0.00001)(X)\n",
    "    X=ZeroPadding2D((1,1))(X)\n",
    "    X=MaxPooling2D((3,3),strides=2)(X)\n",
    "    \n",
    "    X=inception_block3a(X)\n",
    "    X=inception_block3b(X)\n",
    "    X=inception_block3c(X)\n",
    "    \n",
    "    X=inception_block4a(X)\n",
    "    X=inception_block4b(X)\n",
    "    X=inception_block4e(X)\n",
    "    \n",
    "    X=inception_block5a(X)\n",
    "    X=inception_block5b(X)\n",
    "    \n",
    "    X=AveragePooling2D((3,3))(X)\n",
    "    X=Flatten()(X)\n",
    "    X=Dense(128)(X)\n",
    "    X=Lambda(lambda x: K.l2_normalize(x,axis=1))(X)\n",
    "    \n",
    "    facenet=Model(inputs=I,outputs=X,name=\"facenet\")\n",
    "    return facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def tripletloss(inputs,margin):\n",
    "\n",
    "    d_pos = K.sum(K.square(anchor - positive), -1)\n",
    "    d_neg = K.sum(K.square(anchor - negative), -1)\n",
    "\n",
    "    loss = K.maximum(0., margin + d_pos - d_neg)\n",
    "    loss = K.mean(loss)\n",
    "    return loss'''\n",
    "# not original\n",
    "#Code borrowed from the post https://omoindrot.github.io/triplet-loss\n",
    "def _pairwise_distances(embeddings, squared=False):\n",
    "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
    "    \"\"\"\n",
    "    # Get the dot product between all embeddings\n",
    "    # shape (batch_size, batch_size)\n",
    "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
    "\n",
    "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
    "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
    "    # shape (batch_size,)\n",
    "    square_norm = tf.diag_part(dot_product)\n",
    "\n",
    "    # Compute the pairwise distance matrix as we have:\n",
    "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
    "    # shape (batch_size, batch_size)\n",
    "    distances = tf.expand_dims(square_norm, 0) - 2.0 * dot_product + tf.expand_dims(square_norm, 1)\n",
    "\n",
    "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
    "    distances = tf.maximum(distances, 0.0)\n",
    "\n",
    "    if not squared:\n",
    "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
    "        # we need to add a small epsilon where distances == 0.0\n",
    "        mask = tf.to_float(tf.equal(distances, 0.0))\n",
    "        distances = distances + mask * 1e-16\n",
    "\n",
    "        distances = tf.sqrt(distances)\n",
    "\n",
    "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
    "        distances = distances * (1.0 - mask)\n",
    "\n",
    "    return distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_triplet_mask(labels):\n",
    "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
    "    A triplet (i, j, k) is valid if:\n",
    "        - i, j, k are distinct\n",
    "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    Args:\n",
    "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Check that i, j and k are distinct\n",
    "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
    "    indices_not_equal = tf.logical_not(indices_equal)\n",
    "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
    "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
    "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
    "\n",
    "    distinct_indices = tf.logical_and(tf.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
    "\n",
    "\n",
    "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
    "    label_equal = tf.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
    "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
    "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
    "\n",
    "    valid_labels = tf.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = tf.logical_and(distinct_indices, valid_labels)\n",
    "\n",
    "    return mask\n",
    "    \n",
    "def batch_all_triplet_loss(labels, embeddings, margin=0.5, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    We generate all the valid triplets and average the loss over the positive ones.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
    "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n",
    "\n",
    "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
    "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
    "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
    "    # and the 2nd (batch_size, 1, batch_size)\n",
    "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
    "\n",
    "    # Put to zero the invalid triplets\n",
    "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
    "    mask = _get_triplet_mask(labels)\n",
    "    mask = tf.to_float(mask)\n",
    "    triplet_loss = tf.multiply(mask, triplet_loss)\n",
    "\n",
    "    # Remove negative losses (i.e. the easy triplets)\n",
    "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
    "\n",
    "    # Count number of positive triplets (where triplet_loss > 0)\n",
    "    valid_triplets = tf.to_float(tf.greater(triplet_loss, 1e-16))\n",
    "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
    "    num_valid_triplets = tf.reduce_sum(mask)\n",
    "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
    "\n",
    "    # Get final mean triplet loss over the positive valid triplets\n",
    "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
    "\n",
    "    return triplet_loss\n",
    "\n",
    "def batch_hard_triplet_loss(labels, embeddings, margin, squared=False):\n",
    "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
    "\n",
    "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
    "\n",
    "    Args:\n",
    "        labels: labels of the batch, of size (batch_size,)\n",
    "        embeddings: tensor of shape (batch_size, embed_dim)\n",
    "        margin: margin for triplet loss\n",
    "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
    "                 If false, output is the pairwise euclidean distance matrix.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss: scalar tensor containing the triplet loss\n",
    "    \"\"\"\n",
    "    # Get the pairwise distance matrix\n",
    "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
    "\n",
    "    # For each anchor, get the hardest positive\n",
    "    # First, we need to get a mask for every valid positive (they should have same label)\n",
    "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
    "    mask_anchor_positive = tf.to_float(mask_anchor_positive)\n",
    "\n",
    "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
    "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
    "\n",
    "    # shape (batch_size, 1)\n",
    "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
    "\n",
    "    # For each anchor, get the hardest negative\n",
    "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
    "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
    "    mask_anchor_negative = tf.to_float(mask_anchor_negative)\n",
    "\n",
    "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
    "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
    "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
    "\n",
    "    # shape (batch_size,)\n",
    "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
    "\n",
    "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
    "\n",
    "    # Get final mean triplet loss\n",
    "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "    return triplet_loss\n",
    "#need to add two functions for batch hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =fnmodel((224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=batch_all_triplet_loss,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_36 (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_857 (Conv2D)             (None, 112, 112, 64) 9472        input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_471 (ZeroPadding (None, 114, 114, 64) 0           conv2d_857[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_195 (MaxPooling2D (None, 56, 56, 64)   0           zero_padding2d_471[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_765 (BatchN (None, 56, 56, 64)   224         max_pooling2d_195[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_858 (Conv2D)             (None, 56, 56, 64)   4160        batch_normalization_765[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_859 (Conv2D)             (None, 54, 54, 192)  110784      conv2d_858[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_766 (BatchN (None, 54, 54, 192)  216         conv2d_859[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_472 (ZeroPadding (None, 56, 56, 192)  0           batch_normalization_766[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_196 (MaxPooling2D (None, 27, 27, 192)  0           zero_padding2d_472[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_860 (Conv2D)             (None, 27, 27, 96)   18528       max_pooling2d_196[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_862 (Conv2D)             (None, 27, 27, 16)   3088        max_pooling2d_196[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_767 (BatchN (None, 27, 27, 96)   108         conv2d_860[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_769 (BatchN (None, 27, 27, 16)   108         conv2d_862[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_752 (Activation)     (None, 27, 27, 96)   0           batch_normalization_767[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_754 (Activation)     (None, 27, 27, 16)   0           batch_normalization_769[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_197 (MaxPooling2D (None, 13, 13, 192)  0           max_pooling2d_196[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_473 (ZeroPadding (None, 29, 29, 96)   0           activation_752[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_474 (ZeroPadding (None, 31, 31, 16)   0           activation_754[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_864 (Conv2D)             (None, 13, 13, 32)   6176        max_pooling2d_197[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_861 (Conv2D)             (None, 27, 27, 128)  110720      zero_padding2d_473[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_863 (Conv2D)             (None, 27, 27, 32)   12832       zero_padding2d_474[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_771 (BatchN (None, 13, 13, 32)   52          conv2d_864[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_865 (Conv2D)             (None, 27, 27, 64)   12352       max_pooling2d_196[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_768 (BatchN (None, 27, 27, 128)  108         conv2d_861[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_770 (BatchN (None, 27, 27, 32)   108         conv2d_863[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_756 (Activation)     (None, 13, 13, 32)   0           batch_normalization_771[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_772 (BatchN (None, 27, 27, 64)   108         conv2d_865[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_753 (Activation)     (None, 27, 27, 128)  0           batch_normalization_768[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_755 (Activation)     (None, 27, 27, 32)   0           batch_normalization_770[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_475 (ZeroPadding (None, 27, 27, 32)   0           activation_756[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_757 (Activation)     (None, 27, 27, 64)   0           batch_normalization_772[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_132 (Concatenate)   (None, 27, 27, 256)  0           activation_753[0][0]             \n",
      "                                                                 activation_755[0][0]             \n",
      "                                                                 zero_padding2d_475[0][0]         \n",
      "                                                                 activation_757[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_866 (Conv2D)             (None, 27, 27, 96)   24672       concatenate_132[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_868 (Conv2D)             (None, 27, 27, 32)   8224        concatenate_132[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_773 (BatchN (None, 27, 27, 96)   108         conv2d_866[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_775 (BatchN (None, 27, 27, 32)   108         conv2d_868[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_758 (Activation)     (None, 27, 27, 96)   0           batch_normalization_773[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_760 (Activation)     (None, 27, 27, 32)   0           batch_normalization_775[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_198 (MaxPooling2D (None, 13, 13, 256)  0           concatenate_132[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_476 (ZeroPadding (None, 29, 29, 96)   0           activation_758[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_477 (ZeroPadding (None, 31, 31, 32)   0           activation_760[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_870 (Conv2D)             (None, 13, 13, 64)   16448       max_pooling2d_198[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_867 (Conv2D)             (None, 27, 27, 128)  110720      zero_padding2d_476[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_869 (Conv2D)             (None, 27, 27, 64)   51264       zero_padding2d_477[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_777 (BatchN (None, 13, 13, 64)   52          conv2d_870[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_871 (Conv2D)             (None, 27, 27, 64)   16448       concatenate_132[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_774 (BatchN (None, 27, 27, 128)  108         conv2d_867[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_776 (BatchN (None, 27, 27, 64)   108         conv2d_869[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_762 (Activation)     (None, 13, 13, 64)   0           batch_normalization_777[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_778 (BatchN (None, 27, 27, 64)   108         conv2d_871[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_759 (Activation)     (None, 27, 27, 128)  0           batch_normalization_774[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_761 (Activation)     (None, 27, 27, 64)   0           batch_normalization_776[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_478 (ZeroPadding (None, 27, 27, 64)   0           activation_762[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_763 (Activation)     (None, 27, 27, 64)   0           batch_normalization_778[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_133 (Concatenate)   (None, 27, 27, 320)  0           activation_759[0][0]             \n",
      "                                                                 activation_761[0][0]             \n",
      "                                                                 zero_padding2d_478[0][0]         \n",
      "                                                                 activation_763[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_872 (Conv2D)             (None, 27, 27, 128)  41088       concatenate_133[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_874 (Conv2D)             (None, 27, 27, 32)   10272       concatenate_133[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_779 (BatchN (None, 27, 27, 128)  108         conv2d_872[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_781 (BatchN (None, 27, 27, 32)   108         conv2d_874[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_764 (Activation)     (None, 27, 27, 128)  0           batch_normalization_779[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_766 (Activation)     (None, 27, 27, 32)   0           batch_normalization_781[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_479 (ZeroPadding (None, 29, 29, 128)  0           activation_764[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_480 (ZeroPadding (None, 31, 31, 32)   0           activation_766[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_873 (Conv2D)             (None, 14, 14, 256)  295168      zero_padding2d_479[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_875 (Conv2D)             (None, 14, 14, 64)   51264       zero_padding2d_480[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_780 (BatchN (None, 14, 14, 256)  56          conv2d_873[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_782 (BatchN (None, 14, 14, 64)   56          conv2d_875[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_199 (MaxPooling2D (None, 13, 13, 320)  0           concatenate_133[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_765 (Activation)     (None, 14, 14, 256)  0           batch_normalization_780[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_767 (Activation)     (None, 14, 14, 64)   0           batch_normalization_782[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_481 (ZeroPadding (None, 14, 14, 320)  0           max_pooling2d_199[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_134 (Concatenate)   (None, 14, 14, 640)  0           activation_765[0][0]             \n",
      "                                                                 activation_767[0][0]             \n",
      "                                                                 zero_padding2d_481[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_876 (Conv2D)             (None, 14, 14, 96)   61536       concatenate_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_878 (Conv2D)             (None, 14, 14, 32)   20512       concatenate_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_783 (BatchN (None, 14, 14, 96)   56          conv2d_876[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_785 (BatchN (None, 14, 14, 32)   56          conv2d_878[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 14, 14, 640)  0           concatenate_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_768 (Activation)     (None, 14, 14, 96)   0           batch_normalization_783[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_770 (Activation)     (None, 14, 14, 32)   0           batch_normalization_785[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePo (None, 6, 6, 640)    0           lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_482 (ZeroPadding (None, 16, 16, 96)   0           activation_768[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_483 (ZeroPadding (None, 18, 18, 32)   0           activation_770[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 6, 6, 640)    0           average_pooling2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_877 (Conv2D)             (None, 14, 14, 192)  166080      zero_padding2d_482[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_879 (Conv2D)             (None, 14, 14, 64)   51264       zero_padding2d_483[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_880 (Conv2D)             (None, 6, 6, 128)    82048       lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_881 (Conv2D)             (None, 14, 14, 256)  164096      concatenate_134[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_784 (BatchN (None, 14, 14, 192)  56          conv2d_877[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_786 (BatchN (None, 14, 14, 64)   56          conv2d_879[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_484 (ZeroPadding (None, 14, 14, 128)  0           conv2d_880[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_787 (BatchN (None, 14, 14, 256)  56          conv2d_881[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_769 (Activation)     (None, 14, 14, 192)  0           batch_normalization_784[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_771 (Activation)     (None, 14, 14, 64)   0           batch_normalization_786[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_772 (Activation)     (None, 14, 14, 128)  0           zero_padding2d_484[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_773 (Activation)     (None, 14, 14, 256)  0           batch_normalization_787[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_135 (Concatenate)   (None, 14, 14, 640)  0           activation_769[0][0]             \n",
      "                                                                 activation_771[0][0]             \n",
      "                                                                 activation_772[0][0]             \n",
      "                                                                 activation_773[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_882 (Conv2D)             (None, 14, 14, 112)  71792       concatenate_135[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_884 (Conv2D)             (None, 14, 14, 32)   20512       concatenate_135[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_788 (BatchN (None, 14, 14, 112)  56          conv2d_882[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_790 (BatchN (None, 14, 14, 32)   56          conv2d_884[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 14, 14, 640)  0           concatenate_135[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_774 (Activation)     (None, 14, 14, 112)  0           batch_normalization_788[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_776 (Activation)     (None, 14, 14, 32)   0           batch_normalization_790[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_20 (AveragePo (None, 6, 6, 640)    0           lambda_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_485 (ZeroPadding (None, 16, 16, 112)  0           activation_774[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_486 (ZeroPadding (None, 18, 18, 32)   0           activation_776[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 6, 6, 640)    0           average_pooling2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_883 (Conv2D)             (None, 14, 14, 224)  226016      zero_padding2d_485[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_885 (Conv2D)             (None, 14, 14, 64)   51264       zero_padding2d_486[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_886 (Conv2D)             (None, 6, 6, 128)    82048       lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_887 (Conv2D)             (None, 14, 14, 224)  143584      concatenate_135[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_789 (BatchN (None, 14, 14, 224)  56          conv2d_883[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_791 (BatchN (None, 14, 14, 64)   56          conv2d_885[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_487 (ZeroPadding (None, 14, 14, 128)  0           conv2d_886[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_792 (BatchN (None, 14, 14, 224)  56          conv2d_887[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_775 (Activation)     (None, 14, 14, 224)  0           batch_normalization_789[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_777 (Activation)     (None, 14, 14, 64)   0           batch_normalization_791[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_778 (Activation)     (None, 14, 14, 128)  0           zero_padding2d_487[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_779 (Activation)     (None, 14, 14, 224)  0           batch_normalization_792[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_136 (Concatenate)   (None, 14, 14, 640)  0           activation_775[0][0]             \n",
      "                                                                 activation_777[0][0]             \n",
      "                                                                 activation_778[0][0]             \n",
      "                                                                 activation_779[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_888 (Conv2D)             (None, 14, 14, 160)  102560      concatenate_136[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_890 (Conv2D)             (None, 14, 14, 64)   41024       concatenate_136[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_793 (BatchN (None, 14, 14, 160)  56          conv2d_888[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_795 (BatchN (None, 14, 14, 64)   56          conv2d_890[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_780 (Activation)     (None, 14, 14, 160)  0           batch_normalization_793[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_782 (Activation)     (None, 14, 14, 64)   0           batch_normalization_795[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_488 (ZeroPadding (None, 16, 16, 160)  0           activation_780[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_489 (ZeroPadding (None, 18, 18, 64)   0           activation_782[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_889 (Conv2D)             (None, 7, 7, 256)    368896      zero_padding2d_488[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_891 (Conv2D)             (None, 7, 7, 128)    204928      zero_padding2d_489[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_200 (MaxPooling2D (None, 6, 6, 640)    0           concatenate_136[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_794 (BatchN (None, 7, 7, 256)    28          conv2d_889[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_796 (BatchN (None, 7, 7, 128)    28          conv2d_891[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_490 (ZeroPadding (None, 7, 7, 640)    0           max_pooling2d_200[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_781 (Activation)     (None, 7, 7, 256)    0           batch_normalization_794[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_783 (Activation)     (None, 7, 7, 128)    0           batch_normalization_796[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_784 (Activation)     (None, 7, 7, 640)    0           zero_padding2d_490[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_137 (Concatenate)   (None, 7, 7, 1024)   0           activation_781[0][0]             \n",
      "                                                                 activation_783[0][0]             \n",
      "                                                                 activation_784[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_892 (Conv2D)             (None, 7, 7, 192)    196800      concatenate_137[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_894 (Conv2D)             (None, 7, 7, 48)     49200       concatenate_137[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_797 (BatchN (None, 7, 7, 192)    28          conv2d_892[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_799 (BatchN (None, 7, 7, 48)     28          conv2d_894[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 7, 7, 1024)   0           concatenate_137[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_785 (Activation)     (None, 7, 7, 192)    0           batch_normalization_797[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_787 (Activation)     (None, 7, 7, 48)     0           batch_normalization_799[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_21 (AveragePo (None, 3, 3, 1024)   0           lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_491 (ZeroPadding (None, 9, 9, 192)    0           activation_785[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_492 (ZeroPadding (None, 11, 11, 48)   0           activation_787[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 3, 3, 1024)   0           average_pooling2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_893 (Conv2D)             (None, 7, 7, 384)    663936      zero_padding2d_491[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_895 (Conv2D)             (None, 7, 7, 128)    153728      zero_padding2d_492[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_896 (Conv2D)             (None, 3, 3, 128)    131200      lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_897 (Conv2D)             (None, 7, 7, 384)    393600      concatenate_137[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_798 (BatchN (None, 7, 7, 384)    28          conv2d_893[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_800 (BatchN (None, 7, 7, 128)    28          conv2d_895[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_493 (ZeroPadding (None, 7, 7, 128)    0           conv2d_896[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_801 (BatchN (None, 7, 7, 384)    28          conv2d_897[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_786 (Activation)     (None, 7, 7, 384)    0           batch_normalization_798[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_788 (Activation)     (None, 7, 7, 128)    0           batch_normalization_800[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_789 (Activation)     (None, 7, 7, 128)    0           zero_padding2d_493[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_790 (Activation)     (None, 7, 7, 384)    0           batch_normalization_801[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_138 (Concatenate)   (None, 7, 7, 1024)   0           activation_786[0][0]             \n",
      "                                                                 activation_788[0][0]             \n",
      "                                                                 activation_789[0][0]             \n",
      "                                                                 activation_790[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_898 (Conv2D)             (None, 7, 7, 192)    196800      concatenate_138[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_900 (Conv2D)             (None, 7, 7, 48)     49200       concatenate_138[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_802 (BatchN (None, 7, 7, 192)    28          conv2d_898[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_804 (BatchN (None, 7, 7, 48)     28          conv2d_900[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_791 (Activation)     (None, 7, 7, 192)    0           batch_normalization_802[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_793 (Activation)     (None, 7, 7, 48)     0           batch_normalization_804[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_494 (ZeroPadding (None, 9, 9, 192)    0           activation_791[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_495 (ZeroPadding (None, 11, 11, 48)   0           activation_793[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_201 (MaxPooling2D (None, 3, 3, 1024)   0           concatenate_138[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_899 (Conv2D)             (None, 7, 7, 384)    663936      zero_padding2d_494[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_901 (Conv2D)             (None, 7, 7, 128)    153728      zero_padding2d_495[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_902 (Conv2D)             (None, 3, 3, 128)    131200      max_pooling2d_201[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_903 (Conv2D)             (None, 7, 7, 384)    393600      concatenate_138[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_803 (BatchN (None, 7, 7, 384)    28          conv2d_899[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_805 (BatchN (None, 7, 7, 128)    28          conv2d_901[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_496 (ZeroPadding (None, 7, 7, 128)    0           conv2d_902[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_806 (BatchN (None, 7, 7, 384)    28          conv2d_903[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_792 (Activation)     (None, 7, 7, 384)    0           batch_normalization_803[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_794 (Activation)     (None, 7, 7, 128)    0           batch_normalization_805[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_795 (Activation)     (None, 7, 7, 128)    0           zero_padding2d_496[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_796 (Activation)     (None, 7, 7, 384)    0           batch_normalization_806[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_139 (Concatenate)   (None, 7, 7, 1024)   0           activation_792[0][0]             \n",
      "                                                                 activation_794[0][0]             \n",
      "                                                                 activation_795[0][0]             \n",
      "                                                                 activation_796[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_22 (AveragePo (None, 2, 2, 1024)   0           concatenate_139[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 4096)         0           average_pooling2d_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          524416      flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 128)          0           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 6,476,144\n",
      "Trainable params: 6,474,664\n",
      "Non-trainable params: 1,480\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-26fdd02dfed0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnamesm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnamesf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdatasetf_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50824\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdatasetm_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m53015\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdatasetf_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50824\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import io\n",
    "namesm=[]\n",
    "namesf=[]\n",
    "datasetf_images = np.empty((50824,224,224,3),dtype='int')\n",
    "datasetm_images = np.empty((53015,224,224,3),dtype='int')\n",
    "datasetf_names=np.empty(50824)\n",
    "datasetm_names= np.empty(53015)\n",
    "with open('/home/sumanth/Documents/Files/CV/SBI-Stockroom-PS/faceScrub/actors_names.txt') as nfilem: \n",
    "    for line in nfilem:\n",
    "        namesm.append(line)\n",
    "i=0\n",
    "k=0\n",
    "with open('/home/sumanth/Documents/Files/CV/SBI-Stockroom-PS/faceScrub/actors_links.txt') as lfilem:\n",
    "    for line in lfilem :\n",
    "        img = io.imread(line)\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "        img = cv2.resize(img,(224,224))\n",
    "        datasetm_images[i] = img\n",
    "        datasetm_names[i] = k\n",
    "        if namesm[i+1]!=namesm[i]:\n",
    "            k+=1\n",
    "        i+=1\n",
    "with open('/home/sumanth/Documents/Files/CV/SBI-Stockroom-PS/faceScrub/actresses_names.txt') as nfilef: \n",
    "    for line in nfilef:\n",
    "        namesf.append(line)\n",
    "i=0 \n",
    "k=0\n",
    "with open('/home/sumanth/Documents/Files/CV/SBI-Stockroom-PS/faceScrub/actresses_links.txt') as lfilef:\n",
    "    for line in lfilef :\n",
    "        img = io.imread(line)\n",
    "        img = cv2.resize(img,(224,224))\n",
    "        img = cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n",
    "        datasetf_images[i] = img\n",
    "        datasetf_names[i] = k\n",
    "        if namesf[i+1]!=namesf[i]:\n",
    "            k+=1\n",
    "        i+=1\n",
    "datasetmtrain_images= datasetm_images[0:40000]\n",
    "datasetftrain_images = datasetf_images[0:40000]\n",
    "train_images=np.concatenate((datasetmtrain_images,datasetftrain_images),axis=0)\n",
    "datasetmtrain_names = datasetm_names[0:40000]\n",
    "datasetftrain_names = datasetf_names[0:40000]\n",
    "train_names=np.concatenate((datasetmtrain_names,datasetftrain_names),axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasetmtrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-09abc11eb647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatasetmtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'datasetmtrain' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
